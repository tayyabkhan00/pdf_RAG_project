# ğŸ“„ PDF Q&A Chatbot using RAG (Gemini + LangChain + FAISS)

A **Retrieval-Augmented Generation (RAG)** based PDF Question Answering system that allows users to ask questions from a PDF document and get **accurate, context-aware answers** using **Google Gemini**, **LangChain**, and **FAISS**, with a **Streamlit UI and streaming responses**.

This project demonstrates a **production-style GenAI pipeline**, not just a demo.

---

## ğŸš€ Features

- ğŸ“„ Load and process PDF documents
- âœ‚ï¸ Intelligent text chunking with overlap
- ğŸ§  Vector storage using FAISS
- ğŸ” Semantic search using embeddings
- ğŸ¤– Gemini-powered answers (grounded in PDF content)
- ğŸ”„ Retrieval-Augmented Generation (RAG)
- ğŸ’¬ Chat-style Streamlit UI
- âš¡ Streaming responses (ChatGPT-like typing)
- ğŸ” Secure API key handling using `.env`

---

## ğŸ§  How RAG Works (High-Level)
```
PDF Document
â†“
Text Chunking
â†“
Gemini Embeddings
â†“
FAISS Vector Store
â†“
User Question
â†“
Similarity Search
â†“
Relevant Context
â†“
Gemini LLM
â†“
Answer (Streamed)
```

---

## ğŸ› ï¸ Tech Stack

- **Python**
- **Google Gemini (LLM + Embeddings)**
- **LangChain (LCEL, modern APIs)**
- **FAISS** (Vector Database)
- **Streamlit** (UI)
- **python-dotenv** (Environment variables)

---

## ğŸ“ Project Structure
```
PDF_RAG_PROJECT/
â”‚â”€â”€ data/
â”‚ â””â”€â”€ sample.pdf
â”‚â”€â”€ vectorstore/
â”‚ â”œâ”€â”€ index.faiss
â”‚ â””â”€â”€ index.pkl
â”‚â”€â”€ ingest.py # PDF ingestion & vector creation
â”‚â”€â”€ app.py # Streamlit UI + streaming RAG
â”‚â”€â”€ .env # API key (ignored by git)
â”‚â”€â”€ .gitignore
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
```

---

## ğŸ” Environment Setup

### 1ï¸âƒ£ Create a `.env` file

```env
GOOGLE_API_KEY=your_gemini_api_key_here
```
<br>âš ï¸ Never commit .env to GitHub<br>
Ensure .gitignore contains:
```
.env
```
## ğŸ“¦ Installation

2ï¸âƒ£ Install dependencies
```
pip install -r requirements.txt
```
requirements.txt
```
streamlit
python-dotenv
langchain
langchain-core
langchain-community
langchain-google-genai
langchain-text-splitters
faiss-cpu
```
## ğŸ§¾ Step 1: Ingest the PDF

This step:
- Loads the PDF
- Splits text into chunks
- Generates embeddings
- Stores vectors in FAISS
```
python ingest.py
```
Expected output:
```
âœ… PDF ingested successfully
```
A vectorstore/ folder will be created.

## ğŸ’¬ Step 2: Run the Chatbot UI
```
streamlit run app.py
```
The app will open in your browser automatically.

## ğŸ§ª Example Questions You Can Ask

- What is the main problem this paper addresses?
- Explain the proposed methodology.
- How does RAG improve reliability?
- What evaluation results are reported?
- What are the limitations of this approach?

**âŒ Out-of-scope questions (correctly handled):**

- What is the capital of France?
<br>â¡ï¸ Response:<br>
```
I donâ€™t know.
```
## ğŸ”¥ Key Highlights

- Uses modern LangChain LCEL (no deprecated chains)
- Prevents hallucinations by grounding answers in retrieved context
- Streams responses token-by-token for better UX
- Secure and production-aligned architecture
- Resume and interview-ready GenAI project

## ğŸ“ˆ Possible Enhancements

- ğŸ“Œ Show source citations (page numbers)
- ğŸ§  Add conversational memory
- ğŸ“¤ Upload PDFs dynamically
- ğŸŒ Deploy on Streamlit Cloud / Hugging Face Spaces
- ğŸ” Add hybrid search (BM25 + vectors)

## ğŸ¯ Use Cases

- Research paper Q&A
- Internal knowledge base chatbot
- Legal / policy document assistant
- Academic & technical document analysis

## ğŸ‘¨â€ğŸ’» Author

<br>Tayyab Khan<br>
<br>BTech in AI & Data Science<br>
Aspiring GenAI / AI Engineer

## â­ Final Note

This project demonstrates real-world GenAI engineering skills, including:

- RAG architecture
- Vector databases
- LLM orchestration
- Secure API handling
- Streaming UI

**If you find this useful, feel free to â­ star the repository!**
